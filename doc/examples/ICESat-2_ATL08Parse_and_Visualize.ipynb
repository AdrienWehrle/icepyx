{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse ATL08 HDF File \n",
    "1. This notebook is designed to parse ATL08 HDF file And convert to CSV and Shapefile (actually Geopackage!!)  \n",
    "2. Quick data inspection and visualization \n",
    "\n",
    "Bidhya N Yadav   \n",
    "Oct 17, 2020  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.figsize': (15, 8), 'font.size': 10})\n",
    "\n",
    "from shapely.geometry import Point#, Polygon, mapping\n",
    "import h5py\n",
    "from astropy.time import Time\n",
    "\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import geoviews as gv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add code to download data\n",
    "If you need to download data   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path to where you have downloaded ATL08 HDF files\n",
    "# path = f'D:/icesat2/download' #path = './download' test_ATL08\n",
    "path = '../../../download' #change to to suit your path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser to Convert ATL08 HDF file to CSV and Geopackage/Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gps2dyr(time, offset = 0):\n",
    "    \"\"\" Converte GPS time to decimal years. Helper function\"\"\"\n",
    "    time = time + offset\n",
    "    gps_time = Time(time, format='gps')#.decimalyear\n",
    "    iso_time = Time(gps_time, format='iso')\n",
    "    iso_time = iso_time.value\n",
    "    dt = pd.to_datetime(iso_time)\n",
    "    return dt\n",
    "\n",
    "def read_atl08(icesat2_path, gis_output='shp'):\n",
    "    \"\"\" Read ATL08 HDF file and convert to csv and geopackage.     \n",
    "        Extract variables of interest and separate the ATL08 file \n",
    "        into each beam (ground track) and ascending/descending orbits.\n",
    "        TODO: Append ascending/descending nodes to output name\n",
    "        \n",
    "        All the hdf files inside the specified folder will be parsed\n",
    "        Save in the same location\n",
    "    \"\"\"\n",
    "    files = os.listdir(icesat2_path)\n",
    "    hdf_files = [f for f in files if f.endswith('.h5') and 'ATL08' in f]\n",
    "    print(f'Number of HDF files: {len(hdf_files)}')\n",
    "    for f in hdf_files:\n",
    "        print(f'Processing HDF file: {f}')\n",
    "        hdf_path = f'{icesat2_path}/{f}'\n",
    "        res_dict = {}\n",
    "        meta_dict = {} #These will hold metadata required for scalars per ground-track\n",
    "        group = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "        qual_str_count = ''\n",
    "        with h5py.File(hdf_path, 'r') as fi:\n",
    "            # subset group based on data\n",
    "            group = [g for g in list(fi.keys()) if g in group]\n",
    "            group1 = len(group)\n",
    "            group = [g for g in group if 'land_segments' in fi[f'/{g}']]\n",
    "            group2 = len(group)\n",
    "            if group2<group1:\n",
    "                print.info(f'Non-empty groups: {group2}/{group1}')\n",
    "            # NB: Assert if at least one group present else may be error due to enumeration\n",
    "            if len(group) == 0:\n",
    "                print(f'No ground track data in this file: {f}')\n",
    "                continue\n",
    "            t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:] #scalar 1 value\n",
    "            for k,g in enumerate(group):\n",
    "                # 1) Read in data for a single beam #\n",
    "                lat = fi[f'/{g}/land_segments/latitude']\n",
    "                lon = fi[f'/{g}/land_segments/longitude']\n",
    "                t_dt = fi[f'/{g}/land_segments/delta_time']\n",
    "                layer_flag = fi[f'/{g}/land_segments/layer_flag']\n",
    "                # Extract everything for terrain and canopy variables\n",
    "                terrain_keys = fi[f'{g}/land_segments/terrain'].keys()\n",
    "                terrain_keys = list(terrain_keys)\n",
    "                terrain_dict = {}\n",
    "                for tk in terrain_keys:\n",
    "                    terrain_dict[tk] = fi[f'{g}/land_segments/terrain/{tk}']\n",
    "                #terrain = pd.DataFrame.from_dict(terrain_dict)\n",
    "                # Do the same for Canopy\n",
    "                canopy_keys = fi[f'{g}/land_segments/canopy'].keys()\n",
    "                canopy_keys = list(canopy_keys)\n",
    "                # Remove these two keys as they contain some multivalue tuples which are just no-data in version 002\n",
    "                canopy_keys.remove('canopy_h_metrics')\n",
    "                canopy_keys.remove('canopy_h_metrics_abs')\n",
    "                canopy_dict = {}\n",
    "                for ck in canopy_keys:\n",
    "                    canopy_dict[ck] = fi[f'{g}/land_segments/canopy/{ck}']                    \n",
    "                # Remove the canopy subset flag that seems added in version 3 because this is an array but we can't save array to shapefile\n",
    "                if 'subset_can_flag' in canopy_dict:\n",
    "                    del canopy_dict['subset_can_flag']\n",
    "                if 'subset_te_flag' in terrain_dict:\n",
    "                    del terrain_dict['subset_te_flag']\n",
    "                #Merge two dictionaries (order should be retained; verify when running again wity Python 3.7)\n",
    "                terrain_dict.update(canopy_dict)\n",
    "                \n",
    "                # 2) To Make Pandas dataframe\n",
    "                # Collect everythin into one dictionary\n",
    "                gt_dict = {'lon':lon, 'lat':lat, 't_dt':t_dt, 'layer_flag':layer_flag}\n",
    "                gt_dict.update(terrain_dict)\n",
    "                #df = pd.DataFrame({'lon':lon, 'lat':lat, 'h_li': h_li, 'q_flag':q_flag, 't_dt':t_dt})\n",
    "                df = pd.DataFrame.from_dict(gt_dict)\n",
    "                nan_value =  fi[f'/{g}/land_segments/terrain/h_te_mean'].attrs['_FillValue'][0]\n",
    "                df= df.replace(nan_value, np.nan)\n",
    "\n",
    "                #Convert GPS time to actual time using function\n",
    "                df['t_dt'] = df['t_dt'].apply(gps2dyr, offset=t_ref[0])\n",
    "\n",
    "                all_points = len(df)\n",
    "                if len(df)>0:\n",
    "                    # Assemble ground track into a dictionary, later we convert to csv and shp through df\n",
    "                    res_dict[g] = df\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # Now that ATL08 data from separate ground tracks are in one dict, merge it to df and save to csv/shp\n",
    "            if len(res_dict)>0:\n",
    "                # To guard againt empty result dictionary created with no icesat2 passing the quality control\n",
    "                # 1. Combine Dataframes for each of 6 ground-tracks into single Dataframe\n",
    "                count = 0\n",
    "                for k in res_dict.keys():\n",
    "                    if count == 0:\n",
    "                        df = res_dict[k]\n",
    "                        df['strip'] = k\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        df1 = res_dict[k]\n",
    "                        df1['strip'] = k\n",
    "                        df = pd.concat([df, df1], axis=0)\n",
    "                # Choose filename for csv and shapefile\n",
    "                atl_fname = os.path.splitext(hdf_path)[0].split('/')[-1]\n",
    "                df.to_csv(f'{icesat2_path}/{atl_fname}.csv', index=False)\n",
    "                \n",
    "                # 2. Convert to Geopandas\n",
    "                df['geometry'] = df[['lon', 'lat']].apply(lambda x: Point(x), axis=1)\n",
    "                gdf = gpd.GeoDataFrame(df, geometry='geometry', crs = 'epsg:4326')\n",
    "                if gis_output=='shp':\n",
    "                    gdf['t_dt'] = gdf['t_dt'].dt.strftime('%Y-%m-%d %H:%M:%S.%f') #To prevent DriverSupportError: ESRI Shapefile does not support datetime fields\n",
    "                    gdf.to_file(f'{icesat2_path}/{atl_fname}.shp')\n",
    "                elif gis_output=='gpkg':\n",
    "                    gdf.to_file(f'{icesat2_path}/{atl_fname}.gpkg', driver='GPKG') #ignore fiona error for now\n",
    "            else:\n",
    "                print(f\"\\tNo Ground Track in this HDF file; csv or shp not created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the above function to parse hdf file\n",
    "print(f'Parsing ATL08 hdf file')\n",
    "read_atl08(path, gis_output='gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the parsed csv and gis files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../download' #change to to suit your path\n",
    "files = os.listdir(f'{path}')\n",
    "\n",
    "hdf_files = [f for f in files if f.endswith('.h5') and 'ATL08' in f]\n",
    "shp_files = [f for f in files if f.endswith('.gpkg') and 'ATL08' in f]\n",
    "csv_files = [f for f in files if f.endswith('.csv') and 'ATL08' in f]\n",
    "\n",
    "print('No. of shp files',len(shp_files), len(hdf_files))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv and gis files\n",
    "fname ='processed_ATL08_20190731112820_05100403_003_01' #shp_files[idx].split('.gpkg')[0]\n",
    "df = pd.read_csv(f'{path}/{fname}.csv', parse_dates=True) # Load CSV data\n",
    "df['t_dt'] = pd.to_datetime(df.t_dt) # Convert to pandas native date time format to plotting libraries understand the data\n",
    "\n",
    "gdf = gpd.read_file(f'{path}/{fname}.gpkg', parse_dates=True) # Load GIS file\n",
    "# Get unique reference ground tracks\n",
    "gtls = list(df.strip.unique())\n",
    "print(len(df), gtls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf.plot()\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one Ground Track for interactive visualization example\n",
    "df = df[df.strip == 'gt2l']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualization of Parse Files\n",
    "Because the output files are csv and geopackage you are use any software or GIS program for analysis and visualization. For, now lets exploit the rich Python ecosystem (primarily hvPlot) for this quick visualization to get a sense of our data\n",
    "\n",
    "Map Composing  \n",
    "    - Overlay : *  \n",
    "    - Tile : +  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = gv.tile_sources.ESRI # Add basemap to get sense of where the data is\n",
    "gtracks = gdf.hvplot.points(geo=True, color='strip', alpha=0.7, width=500, height=700) # Plot ground tracks\n",
    "# gtracks = gdf[gdf.strip=='gt2l'].hvplot.points(geo=True, color='strip', alpha=0.7, width=500, height=700) # Or subset a particular track\n",
    "\n",
    "## Plot terrain and canopy data\n",
    "terrain = df.hvplot(y='lat', x='h_te_min', kind='scatter', width=350, height=650, color='brown', s=20, alpha=.9).relabel('Terrain')\n",
    "canopy = df.hvplot(y='lat', x='h_max_canopy_abs', kind='scatter', width=350, height=650, color='green', s=10, alpha=.9, title=f'Elevation', xlabel='meters').relabel('Canopy')\n",
    "# Compose a Plot with with basemap overlaying ground track and another tile with terrain and canopy \n",
    "base * gtracks + terrain * canopy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Explore Other Plots\n",
    "df[['h_canopy_abs', 'h_max_canopy_abs', 'h_te_min']].hvplot.scatter(width=1200, s=10, alpha=.5)\n",
    "# Mean and median terrain variables compare\n",
    "# df.hvplot(y='h_te_mean', x='t_dt', by='strip', kind='scatter', s=10) * df.hvplot(y='h_te_median', x='t_dt', by='strip', kind='scatter', s=10, alpha=.5)\n",
    "\n",
    "# Use Time for x-axis\n",
    "df.hvplot(y='h_te_min', x='t_dt', kind='scatter') * \\\n",
    "df.hvplot(y='h_te_median', x='t_dt', kind='scatter', alpha=.5) * \\\n",
    "df.hvplot(y='h_te_max', x='t_dt', kind='scatter', alpha=.7)\n",
    "\n",
    "df[['h_te_best_fit', 'h_te_interp', 'h_te_max', 'h_te_mean', 'h_te_median', 'h_te_min', 'h_te_mode']].hvplot.scatter(width=800, s=10, alpha=.7) + df[['h_max_canopy', 'h_mean_canopy', 'h_median_canopy', 'h_min_canopy']].hvplot.scatter(width=800, s=10, alpha=.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
